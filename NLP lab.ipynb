{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96ab2bec-eadf-4eb6-a8d0-3508a76f22b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Text: ['nlp', 'preprocess', 'includ', 'token', 'clean', 'text', 'remov', 'stopword']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\dhanu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dhanu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### 1\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download resources once\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text)                          # Tokenization\n",
    "    tokens = [t for t in tokens if t.isalpha()]                # Filtration\n",
    "    tokens = [t for t in tokens if re.fullmatch(r\"[A-Za-z]+\", t)]  # Script validation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [t for t in tokens if t.lower() not in stop_words]    # Stop word removal\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(t) for t in tokens]                   # Stemming\n",
    "\n",
    "text = \"NLP preprocessing includes Tokenization, cleaning the text, removing stopwords, and more!\"\n",
    "print(\"Processed Text:\", preprocess_text(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "950341e2-9735-44cc-a375-fcd0f15f272e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence: 'I love learning'\n",
      "Unigram Probability:  0.0016904583020285497\n",
      "Bigram Probability :  0.3333333333333333\n",
      "Trigram Probability: 0.3333333333333333\n",
      "\n",
      "Sentence: 'language models learn'\n",
      "Unigram Probability:  0.00018782870022539445\n",
      "Bigram Probability :  0.5\n",
      "Trigram Probability: 1.0\n",
      "\n",
      "Sentence: 'learning models love'\n",
      "Unigram Probability:  0.0005634861006761833\n",
      "Bigram Probability :  0\n",
      "Trigram Probability: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\dhanu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### 2 \n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Corpus\n",
    "corpus = \"\"\"\n",
    "I love natural language processing.\n",
    "I love machine learning.\n",
    "Language models learn patterns.\n",
    "I love learning new things.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = word_tokenize(corpus.lower())\n",
    "\n",
    "# Unigram, Bigram, Trigram models\n",
    "unigrams = FreqDist(tokens)\n",
    "bigrams = ConditionalFreqDist(ngrams(tokens, 2))\n",
    "trigrams = ConditionalFreqDist(((a, b), c) for a, b, c in ngrams(tokens, 3))\n",
    "\n",
    "# Probability functions\n",
    "def unigram_prob(w): \n",
    "    return unigrams[w] / unigrams.N()\n",
    "\n",
    "def bigram_prob(w1, w2):\n",
    "    return bigrams[w1][w2] / bigrams[w1].N() if bigrams[w1][w2] else 0\n",
    "\n",
    "def trigram_prob(w1, w2, w3):\n",
    "    return trigrams[(w1, w2)][w3] / trigrams[(w1, w2)].N() if trigrams[(w1, w2)][w3] else 0\n",
    "\n",
    "# Sentence probability\n",
    "def sentence_probability(sentence):\n",
    "    words = word_tokenize(sentence.lower())\n",
    "\n",
    "    uni = 1\n",
    "    for w in words:\n",
    "        uni *= unigram_prob(w)\n",
    "\n",
    "    bi = 1\n",
    "    for w1, w2 in ngrams(words, 2):\n",
    "        bi *= bigram_prob(w1, w2)\n",
    "\n",
    "    tri = 1\n",
    "    for w1, w2, w3 in ngrams(words, 3):\n",
    "        tri *= trigram_prob(w1, w2, w3)\n",
    "\n",
    "    return uni, bi, tri\n",
    "\n",
    "# Test sentences\n",
    "sentences = [\"I love learning\", \"language models learn\", \"learning models love\"]\n",
    "\n",
    "for s in sentences:\n",
    "    up, bp, tp = sentence_probability(s)\n",
    "    print(f\"\\nSentence: '{s}'\")\n",
    "    print(\"Unigram Probability: \", up)\n",
    "    print(\"Bigram Probability : \", bp)\n",
    "    print(\"Trigram Probability:\", tp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35e884db-c5fb-423d-b58d-dc09c9b5a28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========= Minimum Edit Distance Demonstration =========\n",
      "\n",
      "cat → cut\n",
      "MED = 1\n",
      "   _  c  u  t\n",
      "_ [0, 1, 2, 3]\n",
      "c [1, 0, 1, 2]\n",
      "a [2, 1, 1, 2]\n",
      "t [3, 2, 2, 1]\n",
      "\n",
      "cat → cats\n",
      "MED = 1\n",
      "   _  c  a  t  s\n",
      "_ [0, 1, 2, 3, 4]\n",
      "c [1, 0, 1, 2, 3]\n",
      "a [2, 1, 0, 1, 2]\n",
      "t [3, 2, 1, 0, 1]\n",
      "\n",
      "cats → cat\n",
      "MED = 1\n",
      "   _  c  a  t\n",
      "_ [0, 1, 2, 3]\n",
      "c [1, 0, 1, 2]\n",
      "a [2, 1, 0, 1]\n",
      "t [3, 2, 1, 0]\n",
      "s [4, 3, 2, 1]\n",
      "\n",
      "speling → spelling\n",
      "MED = 1\n",
      "   _  s  p  e  l  l  i  n  g\n",
      "_ [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "s [1, 0, 1, 2, 3, 4, 5, 6, 7]\n",
      "p [2, 1, 0, 1, 2, 3, 4, 5, 6]\n",
      "e [3, 2, 1, 0, 1, 2, 3, 4, 5]\n",
      "l [4, 3, 2, 1, 0, 1, 2, 3, 4]\n",
      "i [5, 4, 3, 2, 1, 1, 1, 2, 3]\n",
      "n [6, 5, 4, 3, 2, 2, 2, 1, 2]\n",
      "g [7, 6, 5, 4, 3, 3, 3, 2, 1]\n",
      "\n",
      "flaw → lawn\n",
      "MED = 2\n",
      "   _  l  a  w  n\n",
      "_ [0, 1, 2, 3, 4]\n",
      "f [1, 1, 2, 3, 4]\n",
      "l [2, 1, 2, 3, 4]\n",
      "a [3, 2, 1, 2, 3]\n",
      "w [4, 3, 2, 1, 2]\n",
      "\n",
      "intention → execution\n",
      "MED = 5\n",
      "   _  e  x  e  c  u  t  i  o  n\n",
      "_ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "i [1, 1, 2, 3, 4, 5, 6, 6, 7, 8]\n",
      "n [2, 2, 2, 3, 4, 5, 6, 7, 7, 7]\n",
      "t [3, 3, 3, 3, 4, 5, 5, 6, 7, 8]\n",
      "e [4, 3, 4, 3, 4, 5, 6, 6, 7, 8]\n",
      "n [5, 4, 4, 4, 4, 5, 6, 7, 7, 7]\n",
      "t [6, 5, 5, 5, 5, 5, 5, 6, 7, 8]\n",
      "i [7, 6, 6, 6, 6, 6, 6, 5, 6, 7]\n",
      "o [8, 7, 7, 7, 7, 7, 7, 6, 5, 6]\n",
      "n [9, 8, 8, 8, 8, 8, 8, 7, 6, 5]\n"
     ]
    }
   ],
   "source": [
    "### 3\n",
    "\n",
    "def med(a, b):\n",
    "    m, n = len(a), len(b)\n",
    "    dp = [[i + j if i * j == 0 else 0 for j in range(n + 1)] for i in range(m + 1)]\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            dp[i][j] = min(dp[i-1][j] + 1, dp[i][j-1] + 1, dp[i-1][j-1] + (a[i-1] != b[j-1]))\n",
    "    return dp[-1][-1], dp\n",
    "\n",
    "def show(dp, a, b):\n",
    "    print(\"   \" + \"  \".join(\"_\" + b))\n",
    "    for i, r in enumerate(dp):\n",
    "        print((a[i-1] if i else \"_\"), r)\n",
    "\n",
    "tests = [\n",
    "    (\"cat\", \"cut\"),\n",
    "    (\"cat\", \"cats\"),\n",
    "    (\"cats\", \"cat\"),\n",
    "    (\"speling\", \"spelling\"),\n",
    "    (\"flaw\", \"lawn\"),\n",
    "    (\"intention\", \"execution\")\n",
    "]\n",
    "\n",
    "print(\"\\n========= Minimum Edit Distance Demonstration =========\")\n",
    "for s1, s2 in tests:\n",
    "    print(f\"\\n{s1} → {s2}\")\n",
    "    d, dp = med(s1, s2)\n",
    "    print(\"MED =\", d)\n",
    "    show(dp, s1, s2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28b16671-d09b-4020-9ee5-452ff03dca76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Comedy | D) = 7.324218750000001e-05\n",
      "P(Action | D) = 0.00017146776406035664\n",
      "Predicted: action\n"
     ]
    }
   ],
   "source": [
    "### 4\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "docs = [\n",
    "    (\"fun couple love love\".split(), \"comedy\"),\n",
    "    (\"fast furious shoot\".split(), \"action\"),\n",
    "    (\"couple fly fast fun fun\".split(), \"comedy\"),\n",
    "    (\"furious shoot shoot fun\".split(), \"action\"),\n",
    "    (\"fly fast shoot love\".split(), \"action\")\n",
    "]\n",
    "D = \"fast couple shoot fly\".split()\n",
    "\n",
    "classes = {\"comedy\", \"action\"}\n",
    "priors = {c: sum(c == cls for _, cls in docs) / len(docs) for c in classes}\n",
    "\n",
    "vocab = {w for words, _ in docs for w in words}; V = len(vocab)\n",
    "wc = {c: Counter() for c in classes}; tw = {c: 0 for c in classes}\n",
    "for words, c in docs: wc[c].update(words); tw[c] += len(words)\n",
    "\n",
    "def P(c):\n",
    "    p = priors[c]\n",
    "    for w in D: p *= (wc[c][w] + 1) / (tw[c] + V)\n",
    "    return p\n",
    "\n",
    "p_comedy, p_action = P(\"comedy\"), P(\"action\")\n",
    "print(f\"P(Comedy | D) = {p_comedy}\")\n",
    "print(f\"P(Action | D) = {p_action}\")\n",
    "print(\"Predicted:\", \"action\" if p_action > p_comedy else \"comedy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dba1a7ec-cb46-4deb-8385-e787647d7a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\dhanu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     C:\\Users\\dhanu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\dhanu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package udhr to\n",
      "[nltk_data]     C:\\Users\\dhanu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package udhr is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\dhanu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\dhanu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\dhanu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown Categories: ['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
      "Brown Words: ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that']\n",
      "Brown Sentence: ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n",
      "Brown Raw:\n",
      " \n",
      "\n",
      "\tThe/at Fulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl said/vbd Friday/nr an/at investigation/nn of/in Atlanta's/np$ recent/jj primary/nn election/nn produced/vbd ``/`` no/at evidence/nn ''/'' tha\n",
      "\n",
      "Inaugural File IDs: ['1789-Washington.txt', '1793-Washington.txt', '1797-Adams.txt', '1801-Jefferson.txt', '1805-Jefferson.txt']\n",
      "Inaugural 2009 Words: ['My', 'fellow', 'citizens', ':', 'I', 'stand', 'here', 'today', 'humbled', 'by', 'the', 'task', 'before', 'us', ',', 'grateful', 'for', 'the', 'trust', 'you']\n",
      "\n",
      "Reuters Categories: ['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa', 'coconut', 'coconut-oil', 'coffee']\n",
      "Reuters Example Words: ['FRENCH', 'FREE', 'MARKET', 'CEREAL', 'EXPORT', 'BIDS', 'DETAILED', 'French', 'operators', 'have', 'requested', 'licences', 'to', 'export', '675']\n",
      "\n",
      "UDHR Languages: ['Abkhaz-Cyrillic+Abkh', 'Abkhaz-UTF8', 'Achehnese-Latin1', 'Achuar-Shiwiar-Latin1', 'Adja-UTF8', 'Afaan_Oromo_Oromiffa-Latin1', 'Afrikaans-Latin1', 'Aguaruna-Latin1', 'Akuapem_Twi-UTF8', 'Albanian_Shqip-Latin1']\n",
      "UDHR English Words: ['Universal', 'Declaration', 'of', 'Human', 'Rights', 'Preamble', 'Whereas', 'recognition', 'of', 'the', 'inherent', 'dignity', 'and', 'of', 'the', 'equal', 'and', 'inalienable', 'rights', 'of']\n",
      "\n",
      "Sports Word Counts: [('the', 4), ('team', 2), ('won', 2), ('match', 2), ('.', 2), ('with', 1), ('excellent', 1), ('performance', 1)]\n",
      "Tech Word Counts: [('technology', 2), ('is', 2), ('.', 2), ('evolving', 1), ('fast', 1), ('artificial', 1), ('intelligence', 1), ('transforming', 1), ('the', 1), ('industry', 1)]\n",
      "\n",
      "Tagged Words: [('The', 'DT'), ('team', 'NN'), ('won', 'VBD'), ('the', 'DT'), ('match', 'NN'), ('.', '.'), ('The', 'DT'), ('team', 'NN'), ('won', 'VBD'), ('the', 'DT')]\n",
      "Tagged Sentence: [('The', 'DT'), ('team', 'NN'), ('won', 'VBD'), ('the', 'DT'), ('match', 'NN'), ('.', '.')]\n",
      "\n",
      "Most Frequent Noun Tags: [('NN', 8), ('NNP', 1)]\n",
      "\n",
      "Rule-based Tagger: [('This', 'NN'), ('is', 'NN'), ('a', 'NN'), ('sample', 'NN'), ('sentence', 'NN')]\n",
      "\n",
      "Unigram Tagger:\n",
      "Accuracy: 0.75\n",
      "Custom Sentence Tagging: [('AI', 'NN'), ('is', 'VBZ'), ('changing', 'NN'), ('the', 'DT'), ('world', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "### 5\n",
    "\n",
    "\n",
    "import os, nltk\n",
    "from nltk.corpus import brown, inaugural, reuters, udhr, PlaintextCorpusReader\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "\n",
    "for p in ['brown','inaugural','reuters','udhr','punkt_tab','averaged_perceptron_tagger_eng','universal_tagset']: nltk.download(p)\n",
    "\n",
    "print(\"Brown Categories:\", brown.categories())\n",
    "print(\"Brown Words:\", brown.words()[:20])\n",
    "print(\"Brown Sentence:\", brown.sents()[0])\n",
    "print(\"Brown Raw:\\n\", brown.raw()[:200])\n",
    "\n",
    "print(\"\\nInaugural File IDs:\", inaugural.fileids()[:5])\n",
    "print(\"Inaugural 2009 Words:\", inaugural.words('2009-Obama.txt')[:20])\n",
    "\n",
    "print(\"\\nReuters Categories:\", reuters.categories()[:10])\n",
    "print(\"Reuters Example Words:\", reuters.words('training/9865')[:15])\n",
    "\n",
    "print(\"\\nUDHR Languages:\", udhr.fileids()[:10])\n",
    "print(\"UDHR English Words:\", udhr.words('English-Latin1')[:20])\n",
    "\n",
    "root=\"mycorpus\"; sp=os.path.join(root,\"sports\"); te=os.path.join(root,\"tech\")\n",
    "os.makedirs(sp,exist_ok=True); os.makedirs(te,exist_ok=True)\n",
    "open(os.path.join(sp,\"sports1.txt\"),\"w\").write(\"The team won the match with excellent performance.\")\n",
    "open(os.path.join(te,\"tech1.txt\"),\"w\").write(\"Artificial intelligence is transforming the technology industry.\")\n",
    "\n",
    "mycorpus=PlaintextCorpusReader(root,r\".*\\.txt\")\n",
    "cfd=ConditionalFreqDist()\n",
    "for fid in mycorpus.fileids():\n",
    "    cat=fid.split('/')[0]\n",
    "    for w in mycorpus.words(fid): cfd[cat][w.lower()]+=1\n",
    "print(\"\\nSports Word Counts:\", cfd[\"sports\"].most_common(10))\n",
    "print(\"Tech Word Counts:\", cfd[\"tech\"].most_common(10))\n",
    "\n",
    "all_words=mycorpus.words()\n",
    "tagged_words=nltk.pos_tag(all_words)\n",
    "tagged_sents=[nltk.pos_tag(s) for s in mycorpus.sents()]\n",
    "print(\"\\nTagged Words:\", tagged_words[:10])\n",
    "print(\"Tagged Sentence:\", tagged_sents[0])\n",
    "\n",
    "noun_tags={\"NN\",\"NNS\",\"NNP\",\"NNPS\"}\n",
    "freq=nltk.FreqDist(t for _,t in tagged_words if t in noun_tags)\n",
    "print(\"\\nMost Frequent Noun Tags:\", freq.most_common())\n",
    "\n",
    "default_tagger=nltk.DefaultTagger(\"NN\")\n",
    "print(\"\\nRule-based Tagger:\", default_tagger.tag([\"This\",\"is\",\"a\",\"sample\",\"sentence\"]))\n",
    "\n",
    "print(\"\\nUnigram Tagger:\")\n",
    "split=int(0.8*len(tagged_sents)); train,test=tagged_sents[:split],tagged_sents[split:]\n",
    "uni=nltk.tag.UnigramTagger(train, backoff=default_tagger)\n",
    "print(\"Accuracy:\", uni.accuracy(test) if test else \"No test sentences available.\")\n",
    "print(\"Custom Sentence Tagging:\", uni.tag(\"AI is changing the world\".split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59d18c38-1bca-4a5f-881f-d6bb11744dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms of 'active':\n",
      "['active', 'active_agent', 'active_voice', 'alive', 'combat-ready', 'dynamic', 'fighting', 'participating']\n",
      "\n",
      "Antonyms of 'active':\n",
      "['dormant', 'extinct', 'inactive', 'passive', 'passive_voice', 'quiet', 'stative']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dhanu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### 6\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "word = \"active\"\n",
    "synonyms = set()\n",
    "antonyms = set()\n",
    "\n",
    "for syn in wordnet.synsets(word):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.add(lemma.name())\n",
    "        if lemma.antonyms():\n",
    "            antonyms.add(lemma.antonyms()[0].name())\n",
    "\n",
    "print(\"Synonyms of 'active':\")\n",
    "print(sorted(synonyms))\n",
    "\n",
    "print(\"\\nAntonyms of 'active':\")\n",
    "print(sorted(antonyms))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbcb7330-100a-4fbc-a49f-ad25b0860736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\dhanu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\dhanu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\dhanu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sentences: 51606\n",
      "HMM Tagger Accuracy: 0.7011378638335324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter a sentence (or 'exit' to quit):  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged Sentence: [('quit', 'VERB')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter a sentence (or 'exit' to quit):  exit\n"
     ]
    }
   ],
   "source": [
    "### 7\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag.hmm import HiddenMarkovModelTrainer\n",
    "\n",
    "for p in ['brown','punkt_tab','universal_tagset']: nltk.download(p)\n",
    "\n",
    "tagged = brown.tagged_sents(tagset='universal'); split = int(0.9*len(tagged))\n",
    "train, test = tagged[:split], tagged[split:]\n",
    "print(\"Training sentences:\", len(train))\n",
    "\n",
    "hmm = HiddenMarkovModelTrainer().train_supervised(train)\n",
    "print(\"HMM Tagger Accuracy:\", hmm.accuracy(test))\n",
    "\n",
    "while True:\n",
    "    s = input(\"\\nEnter a sentence (or 'exit' to quit): \")\n",
    "    if s.lower() == \"exit\": break\n",
    "    print(\"Tagged Sentence:\", hmm.tag(nltk.word_tokenize(s)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d29f1a-236d-4ce9-a347-d0b21aaefcc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
